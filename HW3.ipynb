{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-401.5]\n",
      " [-800. ]]\n",
      "[[160600.00000137]\n",
      " [320400.00000274]]\n"
     ]
    }
   ],
   "source": [
    "A_1 = np.array([[400, -201], [-800, 401]])\n",
    "A_2 = np.array([[401, -201], [-800, 401]])\n",
    "Y = np.array([[200], [400]])\n",
    "\n",
    "x = np.linalg.solve(A_1, Y)\n",
    "y = np.linalg.solve(A_2, Y)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2503.004600350619\n"
     ]
    }
   ],
   "source": [
    "#手动实现\n",
    "eigenvals_A_1 = np.linalg.eigvals(A_1.T @ A_1)\n",
    "My_Cond_A = np.sqrt(np.max(eigenvals_A_1)/np.min(eigenvals_A_1))\n",
    "print(My_Cond_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2503.0046004808573\n"
     ]
    }
   ],
   "source": [
    "#调包\n",
    "Cond_A = np.linalg.cond(A_1)\n",
    "print(Cond_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们都知道，指数分布的均值为 $\\frac{1}{\\lambda}$，方差为 $\\frac{1}{\\lambda^2}$，所以我们可以得到：\n",
    "\n",
    "\\begin{cases}\n",
    "\\frac{1}{\\lambda} = \\frac{1}{n} \\sum_{i=1}^n x_i \\\\\n",
    "\\frac{1}{\\lambda^2} = \\frac{1}{n} \\sum_{i=1}^n x_i^2 \\\\\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果只用一阶矩，则矩估计为：\n",
    "$$\\hat{\\lambda} = \\frac{n}{\\sum_{i=1}^n x_i}$$\n",
    "\n",
    "如果用两阶矩，则矩估计为：\n",
    "$$\\hat{\\lambda} = \\sqrt{\\frac{n}{\\sum_{i=1}^n x_i^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然：\n",
    "\n",
    "$$\n",
    "\\bar {\\hat y} = \\frac{1}{n} \\sum_{i=1}^n \\hat y_i = \\frac{1}{n} \\sum_{i=1}^n x_i^\\prime \\beta + \\frac{1}{n} \\sum_{i=1}^n \\epsilon_i = \\frac{1}{n} \\sum_{i=1}^n x_i^\\prime \\beta = \\bar y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\ln L(\\beta, \\sigma^2) &= -\\frac{n}{2} \\ln (2\\pi) - \\frac{n}{2} \\ln \\sigma^2 - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - x_i^\\prime \\beta)^2 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "对其求导得：\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\ln L(\\beta, \\sigma^2)}{\\partial \\beta} &= \\frac{1}{\\sigma^2} \\sum_{i=1}^n x_i (y_i - x_i^\\prime \\beta) = 0 \\\\\n",
    "\\frac{\\partial \\ln L(\\beta, \\sigma^2)}{\\partial \\sigma^2} &= -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i=1}^n (y_i - x_i^\\prime \\beta)^2 = 0 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "解得：\n",
    "\\begin{align*}\n",
    "\\hat \\beta &= (X^\\prime X)^{-1} X^\\prime y \\\\\n",
    "\\hat \\sigma^2 &= \\frac{1}{n} \\sum_{i=1}^n (y_i - x_i^\\prime \\hat \\beta)^2 \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "工具变量用于解决解释变量的内生性问题，其需要满足两个条件：\n",
    "\n",
    "1、工具变量与解释变量相关（相关性）\n",
    "\n",
    "2、工具变量与误差项不相关（外生性）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我感觉题目没写清楚，我姑且认为这里的交换指的是特征维度上的交换。\n",
    "\n",
    "不妨记$\\tilde X = XP$，其中$P$为一个置换矩阵。\n",
    "\n",
    "则：\n",
    "\\begin{align*}\n",
    "\\beta_{IV} &= (\\tilde X^\\prime X)^{-1} \\tilde X^\\prime Y \\\\\n",
    "&= ((XP)^\\prime X)^{-1} (XP)^\\prime Y \\\\\n",
    "&= (P^\\prime X^\\prime X)^{-1} P^\\prime X^\\prime Y \\\\\n",
    "&= (X^\\prime X)^{-1} {P^\\prime}^{-1} P^\\prime X^\\prime Y \\\\\n",
    "&= (X^\\prime X)^{-1} X^\\prime Y \\\\\n",
    "&= \\beta_{os} \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "n = 50\n",
    "t = np.arange(1, n+1) + np.random.uniform(-0.01, 0.01, n)\n",
    "X = np.array([np.ones_like(t), t, t**2]).copy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 16.628245953376705, 16.628245953376673]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def VIF(X):\n",
    "    vif = []\n",
    "    for i in range(X.shape[1]):\n",
    "        y = X[:, i].copy()\n",
    "        x = np.delete(X, i, axis=1).copy()\n",
    "        r2 = LinearRegression(fit_intercept=False).fit(x, y).score(x, y)\n",
    "        vif.append(1/(1-r2))\n",
    "    return vif\n",
    "\n",
    "VIF(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "故$t$和$t^2$有较强的共线性，我们不妨丢掉$t^2$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.9999999999999998]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_now = np.delete(X, 2, axis=1).copy()\n",
    "VIF(X_now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Ridge回归中：\n",
    "\n",
    "\\begin{align*}\n",
    "\\beta &= argmin_\\beta \\sum_{i=1}^n (y_i - x_i^\\prime \\beta)^2 + \\frac{1}{2}\\lambda \\sum_{j=1}^p \\beta_j^2 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "求导易得：\n",
    "$$\n",
    "\\beta = (X^\\prime X + \\lambda I)^{-1} X^\\prime y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不妨只考虑logit模型，且只考虑二分类问题：\n",
    "\n",
    "我们记$\\sigma(x) = \\frac{1}{1 + e^{-x}}$，其导数为$\\sigma^\\prime(x) = \\sigma(x) (1 - \\sigma(x))$。\n",
    "\n",
    "此时我们需要回归的是：\n",
    "\n",
    "$$\n",
    "\\mathbb P[Y=1|X] = \\sigma(X^\\prime \\beta)\n",
    "$$\n",
    "\n",
    "如果我们认为$Y$服从伯努利分布，则其似然函数为：\n",
    "\\begin{align*}\n",
    "L(\\beta) &= \\prod_{i=1}^n \\mathbb P[Y=y_i|X=x_i] \\\\\n",
    "&= \\prod_{i=1}^n \\mathbb P[Y=1|X=x_i]^{y_i} \\mathbb P[Y=0|X=x_i]^{1-y_i} \\\\\n",
    "&= \\prod_{i=1}^n \\sigma(x_i^\\prime \\beta)^{y_i} (1 - \\sigma(x_i^\\prime \\beta))^{1-y_i} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "其对数似然函数为：\n",
    "\\begin{align*}\n",
    "\\ln L(\\beta) &= \\sum_{i=1}^n y_i \\ln \\sigma(x_i^\\prime \\beta) + (1-y_i) \\ln (1 - \\sigma(x_i^\\prime \\beta)) \\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其导数为:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\ln L(\\beta)}{\\partial \\beta} &= \\sum_{i=1}^n y_i \\frac{\\partial \\ln \\sigma(x_i^\\prime \\beta)}{\\partial \\beta} + (1-y_i) \\frac{\\partial \\ln (1 - \\sigma(x_i^\\prime \\beta))}{\\partial \\beta} \\\\\n",
    "&= \\sum_{i=1}^n y_i \\frac{\\sigma^\\prime(x_i^\\prime \\beta)}{\\sigma(x_i^\\prime \\beta)} x_i + (1-y_i) \\frac{-\\sigma^\\prime(x_i^\\prime \\beta)}{1 - \\sigma(x_i^\\prime \\beta)} x_i \\\\\n",
    "&= \\sum_{i=1}^n y_i (1 - \\sigma(x_i^\\prime \\beta)) x_i - (1-y_i) \\sigma(x_i^\\prime \\beta) x_i \\\\\n",
    "&= \\sum_{i=1}^n (y_i - \\sigma(x_i^\\prime \\beta)) x_i \\\\\n",
    "\\end{align*}\n",
    "\n",
    "进一步其二阶导数为：\n",
    "\\begin{align*}\n",
    "\\frac{\\partial^2 \\ln L(\\beta)}{\\partial \\beta \\partial \\beta^\\prime} &= \\sum_{i=1}^n -\\sigma^\\prime(x_i^\\prime \\beta) x_i x_i^\\prime \\\\\n",
    "\\end{align*}\n",
    "\n",
    "其为负半定矩阵，故其为凹函数，很容易求解其最大值。（具体而言，使用梯度下降或牛顿法即可）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
